{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muddassarsharif/Desktop/muddassar/production/API/env4/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from lib.readfile import ReadFile as rf\n",
    "from lib.handle_null_values import handle_null_values as null\n",
    "from lib.datetime_formatting import DatetimeFormatting as dfmt\n",
    "from lib.split_time_variable import split_time_variable as split_t\n",
    "from lib.correct_variable_types import correct_variable_types as vartype\n",
    "from lib.handle_null_values import handle_null_values as nulval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Read the file\"\"\"\n",
    "\n",
    "r= rf()\n",
    "df= r.read(address=\"lib/data/2018.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Remove column with null values\"\"\"\n",
    "\n",
    "n= null()\n",
    "variables= n.delete_var_with_null_more_than(df, percentage=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"drop unwanted variables for now\"\"\"\n",
    "\n",
    "df2= df.drop(*variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"select important variables\"\"\" \n",
    "\n",
    "important_variables=['year_built', 'bedroom_count', 'bathroom_count','lot_size', 'area', 'city_name', 'community_name', 'covered_parking', 'exterior', 'energy_source','fireplaces_count', 'floor_covering', 'garage','heating_cooling','latitude','longitude', 'property_type', 'roof', 'sold_date', 'sold_price','unfinished_sq_foot', 'year_built', 'zip_code', 'county' ]\n",
    "\n",
    "    \n",
    "important_variables= list(set(important_variables))\n",
    "df3= df2.select(*important_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"concatenate all files\"\"\"\n",
    "\n",
    "# files= [\"09\"]\n",
    "# for f_no in range(10, 20):\n",
    "#     files.append(f_no)    \n",
    "    \n",
    "#     df4= r.read(address=\"lib/data/20\"+str(f_no)+\".csv\")\n",
    "#     df4= df4.select(*important_variables)\n",
    "#     df3 = df3.union(df4)\n",
    "\n",
    "file= \"tbl_property_details_mls_2019\"\n",
    "for f_no in range(1):  \n",
    "    \n",
    "    df4= r.read(address=\"lib/data/\"+file+\".csv\")\n",
    "    df4= df4.select(*important_variables)\n",
    "    df3 = df3.union(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"filter one county from the dataset\"\"\"\n",
    "\n",
    "df5= df3.filter( df3[\"county\"].isin([\"King\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"correct time format\"\"\"\n",
    "\n",
    "f= split_t()\n",
    "\n",
    "#Assuming time variable is in timestamp type\n",
    "df6= f.run(df5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Split time variable\"\"\"\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import isnan, when, count, col, from_unixtime\n",
    "from pyspark.sql.types import DoubleType, TimestampType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "df6_1= df6.withColumn('sold_date', from_unixtime(unix_timestamp('sold_date', \"yyyy-MM-DD\")).cast(TimestampType()))\n",
    "df6_1= df6_1.withColumn('sold_date_year', year('sold_date'))\n",
    "df6_1= df6_1.withColumn('sold_date_month', month('sold_date'))\n",
    "df6_1= df6_1.withColumn('sold_date_day', dayofmonth('sold_date'))\n",
    "# df6_1= df6_1.withColumn('sold_date_day', dayofweek('sold_date'))\n",
    "df6_1= df6_1.drop('sold_date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Correct variable types\"\"\"\n",
    "\n",
    "v= vartype()\n",
    "\n",
    "df7= v.run(df6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"make training and testing files and save them.\"\"\"\n",
    "# df_t0= df7.filter( df7[\"sold_date_year\"]>2017)\n",
    "# df_t= df_t0.filter(df_t0[\"sold_date_year\"]<2019)\n",
    "df_tst0= df7.filter( df7[\"sold_date_year\"]==2019)\n",
    "df_tst= df_tst0.filter(df7[\"sold_date_month\"]>=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3355\n",
      "75209\n",
      "-71854\n"
     ]
    }
   ],
   "source": [
    "print(df_tst.count())\n",
    "print(df7.count())\n",
    "print(df_tst.count()-df7.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71854"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t= df7.exceptAll(df_tst)\n",
    "df_t.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"duplicate 2019 data in the training file\"\"\"\n",
    "df_2019= df_t.filter(df_t[\"sold_date_year\"]==2019)\n",
    "df_2019= df_2019.filter(df_2019[\"sold_date_year\"]<8)\n",
    "\n",
    "for i in range(2):\n",
    "    df_t = df_t.union(df_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7.toPandas().to_excel(\"lib/data/king'S_COUNTY7.0.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2= df_t.toPandas()\n",
    "df_test2= df_tst.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2.to_excel(\"lib/data/king_train_9.xlsx\")\n",
    "df_test2.to_excel(\"lib/data/king_test_9.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the deep learning model and put that on the training\n",
    "\n",
    "# def var_types(df7):\n",
    "#     categorical_variables= []\n",
    "#     numerical_variables=[]\n",
    "#     for dt in df7.dtypes:\n",
    "#         if dt[1]==\"string\":\n",
    "#             categorical_variables.append(dt[0])\n",
    "#         else:\n",
    "#             numerical_variables.append(dt[0])\n",
    "#     return categorical_variables, numerical_variables\n",
    "        \n",
    "# categorical_variables, numerical_variables= var_types(df7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"treat null values using deep learning\"\"\"\n",
    "\n",
    "# # add an ID in the dataset\n",
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql.window import Window as W\n",
    "# from pyspark import SparkContext\n",
    "# from pyspark.sql import SparkSession\n",
    "# from math import *\n",
    "\n",
    "\n",
    "# spark_context = SparkContext.getOrCreate()\n",
    "# spark = SparkSession(spark_context)\n",
    "\n",
    "# windowSpec = W.orderBy(df7.columns[0])\n",
    "# df7= df7.withColumn(\"id\", F.row_number().over(windowSpec))\n",
    "# df8= df7.createOrReplaceTempView(\"customer\")\n",
    "\n",
    "# #use this ID to filter the data and get results.\n",
    "# base_query = \"SELECT * FROM customer where \"\n",
    "# total_rows= df7.count()\n",
    "# each_page= 500\n",
    "# total_pages= ceil(total_rows/each_page)\n",
    "\n",
    "# for p in range(total_pages):\n",
    "#     query= base_query + \"id > \" + str(each_page*p) + \" and id< \" + str(each_page*(p+1))\n",
    "#     df_spark= spark.sql(query)\n",
    "\n",
    "#     df_pandas= df_spark.toPandas()\n",
    "    \n",
    "#     #pass this through all the models in the same order and run it.\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #save the model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the deep learning model and put that on the training\n",
    "categorical_variables= []\n",
    "numerical_variables=[]\n",
    "all_variables= []\n",
    "for dt in df_t.dtypes:\n",
    "    if dt[1]==\"string\":\n",
    "        categorical_variables.append(dt[0])\n",
    "    else:\n",
    "        numerical_variables.append(dt[0])\n",
    "        \n",
    "    all_variables.append(dt[0])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datawig\n",
    "results= {}\n",
    "models={}\n",
    "\n",
    "for c in categorical_variables: \n",
    "    var= all_variables.copy()\n",
    "    var.remove(c)\n",
    "    imputer = datawig.SimpleImputer(\n",
    "        input_columns=var, # column(s) containing information about the column we want to impute\n",
    "        output_column=c, # the column we'd like to impute values for\n",
    "        output_path = 'lib/Kingimputer_models9.0/'+str(c) # stores model data and metrics\n",
    "        )\n",
    "    imputer.fit(train_df=df_train2, num_epochs=5)\n",
    "    models[c]= imputer\n",
    "    #time to test the model with entire file.  \n",
    "    \n",
    "#     results[c] = imputer.predict(df_pandas)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xyz= df_test2\n",
    "for c in categorical_variables:\n",
    "    m= models[c]\n",
    "    xyz= m.predict(xyz)\n",
    "    \n",
    "\n",
    "\n",
    "xyz2= df_train2\n",
    "for c in categorical_variables:\n",
    "    m= models[c]\n",
    "    xyz2= m.predict(xyz2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_extra_categorical_variables(df=None, variables=None):\n",
    "    if variables==None:\n",
    "        varaibles= self.categorical_variables\n",
    "        \n",
    "        #make a list of all the categorical columns. \n",
    "    no_n=[]   \n",
    "\n",
    "        #delete all unnecessary columns, including the one that make no senese.\n",
    "    for cv in variables:\n",
    "        no_n.append(cv+'_imputed_proba')\n",
    "        no_n.append(cv)\n",
    "\n",
    "    df= df.drop(no_n, axis=1)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "xyz111= delete_extra_categorical_variables(xyz, categorical_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run h2o library offfline\n",
    "xyz222= delete_extra_categorical_variables(xyz2, categorical_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz111.to_excel(\"lib/data/King'S_COUNTY_test7.0.xlsx\")\n",
    "xyz222.to_excel(\"lib/data/King'S_COUNTY_train7.0.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xyz111.to_parquet(\"lib/data/King'S_COUNTY_test7.0.parquet\",engine='fastparquet', compression='gzip'))\n",
    "# # xyz222.to_excel(\"lib/data/King'S_COUNTY_train7.0.xlsx\")\n",
    "\n",
    "from fastparquet import write \n",
    "write(\"lib/data/King'S_COUNTY_test7.0.parquet\", xyz111)\n",
    "write(\"lib/data/King'S_COUNTY_train7.0.xlsx\", xyz222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz222.to_parquet(\"lib/data/King'S_COUNTY_train7.0.xlsx\",engine='fastparquet', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write(\"lib/data/King'S_COUNTY_train7.1.parquet\", xyz222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = h2o.import_file(\"lib/data/Snohomish'S_COUNTY_train6.3.csv\")\n",
    "# test = h2o.import_file(\"lib/data/Snohomish'S_COUNTY_test6.3.csv\")\n",
    "\n",
    "train = h2o.H2OFrame(xyz222)\n",
    "test = h2o.H2OFrame(xyz111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify predictors and response\n",
    "x = train.columns\n",
    "y = 'sold_price'\n",
    "x.remove(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run AutoML for 20 base models (limited to 1 hour max runtime by default)\n",
    "aml = H2OAutoML(max_models=20, seed=100)\n",
    "aml.train(x=x, y=y, training_frame=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = aml.leader.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p= preds.as_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "lst_p = list(np.array(p['predict']))\n",
    "lst_a = list(np.array(test.as_data_frame()['sold_price']))\n",
    "\n",
    "new_lst = []\n",
    "mape= []\n",
    "for i in range(len(lst_p)):\n",
    "    \n",
    "    mape.append(float(float(((lst_p[i]-lst_a[i])**2)**(1/2))/lst_a[i]))\n",
    "    new_lst.append(float(lst_p[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re= test.as_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re[\"sold_price_predict\"]= new_lst\n",
    "re[\"APE\"]= mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.to_excel(\"lib/data/POC-draft-King-7.0.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = h2o.save_model(model=aml.leader, path=\"lib/data/mymodel-King7.0\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7.write.parquet(\"lib/data/df7.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the Parquet file created above.\n",
    "# Parquet files are self-describing so the schema is preserved.\n",
    "# The result of loading a parquet file is also a DataFrame.\n",
    "\n",
    "# imports\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from lib.logs import logger\n",
    "\n",
    "spark_context = SparkContext.getOrCreate()\n",
    "spark = SparkSession(spark_context)\n",
    "df71 = spark.read.parquet(\"lib/data/df7.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df71.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz111.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "housing = load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target,\n",
    "                                                    train_size=0.75, test_size=0.25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
